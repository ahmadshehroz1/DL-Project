# Emotion Detection from Micro-Expressions

## Project Overview
This project focuses on **emotion recognition from facial micro-expressions** using **Deep Learning** techniques. Micro-expressions are brief, involuntary facial expressions that reveal genuine emotions and are challenging to detect due to their short duration and subtle intensity.

The project applies a **Semi-Supervised Learning (SSL)** strategy combined with **Knowledge Distillation (KD)** to effectively leverage limited labeled data and improve classification performance. A teacher–student framework is used, where a self-supervised teacher model guides a lightweight student model.

This work was completed as a **Semester Project in Deep Learning**.

---

## Methodology

### 1. Semi-Supervised Learning (SSL)
- A teacher model is trained using **DINO-style self-supervised learning** on unlabeled micro-expression images.
- SSL helps learn strong feature representations without relying entirely on labeled data.

### 2. Teacher Fine-Tuning
- The SSL-trained teacher backbone is fine-tuned using labeled data to adapt it for emotion classification.

### 3. Knowledge Distillation (KD)
- A student model is trained using soft targets generated by the fine-tuned teacher.
- This allows the student to achieve competitive performance with fewer parameters.

---

## Dataset
- **CASME II** and **SMIC** datasets (combined)
- **Total images:** ~18,000
- **Type:** Facial micro-expression images
- **Classes:** Emotion categories from micro-expressions

> Due to size and licensing constraints, the dataset is **not included** in this repository.

---

## Project Structure
```
.
├── main.py                     # Entry point to run the full pipeline
├── scripts/                    # Training scripts for teacher and student
├── models/                     # Saved teacher and student models
├── data/                   # Dataset directory (not included)
├── requirements.txt            # Python dependencies
├── README.md                   # Project documentation
└── .gitignore
```

---

## Environment Setup

### Requirements
- Python 3.9+
- PyTorch
- CUDA-enabled GPU (recommended)

### Hardware Used
- **GPU:** NVIDIA RTX 4050 (6GB VRAM)

### Install Dependencies
```bash
pip install -r requirements.txt
```

---

## How to Run

To execute the complete pipeline (SSL training, teacher fine-tuning, and student distillation), run:

```bash
python main.py
```

The script automatically:
1. Trains the SSL teacher model
2. Fine-tunes the teacher on labeled data
3. Distills knowledge into a student model

---

## Results

### Teacher Model (After Fine-Tuning)
- **Best Validation Accuracy:** ~95.35%

### Student Model (After Knowledge Distillation)
- **Final Validation Accuracy:** ~93.38%

The results show that the student model successfully learns from the teacher and achieves strong performance while being more efficient.

---

## Key Features
- Semi-supervised representation learning
- Knowledge distillation for model compression
- Efficient training with limited labeled data
- GPU-accelerated training using mixed precision

---

## Notes
- Virtual environment and datasets are intentionally excluded from the repository.
- Model checkpoints can be regenerated by running `main.py`.


---

## Acknowledgements
- CASME II Dataset
- SMIC Dataset
- PyTorch and Open-source DL community

